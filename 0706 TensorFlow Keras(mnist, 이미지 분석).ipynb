{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3863b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe951057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5893a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f1faaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(path, kind = 'train'):\n",
    "    labels_path = os.path.join(path, \"%s-labels-idx1-ubyte\"%kind)\n",
    "    images_path = os.path.join(path, \"%s-images-idx3-ubyte\"%kind)\n",
    "    # label\n",
    "    with open(labels_path, \"rb\") as la_path:\n",
    "        magic, n = struct.unpack(\">II\", la_path.read(8))\n",
    "        labels = np.fromfile(la_path, dtype = np.uint8)\n",
    "    \n",
    "    # image\n",
    "    with open(images_path, \"rb\") as img_path:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", img_path.read(16))\n",
    "        images = np.fromfile(img_path, dtype = np.uint8).reshape(len(labels), 28**2)\n",
    "        images = ((images/255)-0.5)*2\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df872939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 784\n",
      "10000 784\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = load_mnist(\"./\", kind = \"train\")\n",
    "X_test, y_test = load_mnist(\"./\", kind = \"t10k\")\n",
    "print(X_train.shape[0], X_train.shape[1])\n",
    "print(X_test.shape[0], X_test.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac2c62a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_vals = np.mean(X_train, axis = 0)\n",
    "std_val = np.std(X_train)\n",
    "X_train_centered = (X_train - mean_vals)/std_val\n",
    "X_test_centered = (X_test - mean_vals)/std_val\n",
    "X_train_centered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5de8587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_onehot = tf.keras.utils.to_categorical(y_train)\n",
    "y_train_onehot[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2a22fcd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4], dtype=uint8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "769f41b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units = 50,\n",
    "        input_dim = X_train_centered.shape[1],\n",
    "        kernel_initializer = 'glorot_uniform',\n",
    "        bias_initializer = 'zeros',\n",
    "        activation = 'tanh'\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "# 두 번째 은닉층(학습층) 앞에서 50개의 모델 유닛이 있었으므로, input_dim 도 이번엔 50개를 학습시킨다.\n",
    "\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units = 50,\n",
    "        input_dim = 50,\n",
    "        kernel_initializer = 'glorot_uniform',\n",
    "        bias_initializer = 'zeros',\n",
    "        activation = 'tanh'\n",
    "        \n",
    "    )\n",
    ")\n",
    "\n",
    "# 세 번째 은닉층(학습층). 마지막 학습층이므로 activation 이 달라짐.(softmax => 이건 이거야! 가 아니라 이게 제일 확률 높은데~? 이거같아!)\n",
    "\n",
    "model.add(\n",
    "    tf.keras.layers.Dense(\n",
    "        units = 10,\n",
    "        input_dim = 50,\n",
    "        kernel_initializer = 'glorot_uniform',\n",
    "        bias_initializer = 'zeros',\n",
    "        activation = 'softmax'\n",
    "        \n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df04a85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 50)                39250     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                510       \n",
      "=================================================================\n",
      "Total params: 42,310\n",
      "Trainable params: 42,310\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e6916558",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_optimizer = tf.keras.optimizers.SGD(lr=0.0001, decay = 1e-7, momentum=0.9)\n",
    "model.compile(optimizer=sgd_optimizer, loss = \"categorical_crossentropy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3a2c5dbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 1.6905 - val_loss: 1.1833\n",
      "Epoch 2/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 1.0610 - val_loss: 0.8588\n",
      "Epoch 3/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.8373 - val_loss: 0.6921\n",
      "Epoch 4/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.7084 - val_loss: 0.5899\n",
      "Epoch 5/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.6242 - val_loss: 0.5212\n",
      "Epoch 6/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.5650 - val_loss: 0.4723\n",
      "Epoch 7/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.5210 - val_loss: 0.4356\n",
      "Epoch 8/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.4872 - val_loss: 0.4074\n",
      "Epoch 9/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.4603 - val_loss: 0.3848\n",
      "Epoch 10/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.4383 - val_loss: 0.3664\n",
      "Epoch 11/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.4199 - val_loss: 0.3509\n",
      "Epoch 12/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.4043 - val_loss: 0.3379\n",
      "Epoch 13/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3909 - val_loss: 0.3267\n",
      "Epoch 14/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3791 - val_loss: 0.3169\n",
      "Epoch 15/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3687 - val_loss: 0.3083\n",
      "Epoch 16/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3594 - val_loss: 0.3006\n",
      "Epoch 17/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3511 - val_loss: 0.2936\n",
      "Epoch 18/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3435 - val_loss: 0.2872\n",
      "Epoch 19/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3365 - val_loss: 0.2814\n",
      "Epoch 20/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3301 - val_loss: 0.2761\n",
      "Epoch 21/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3242 - val_loss: 0.2712\n",
      "Epoch 22/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3186 - val_loss: 0.2667\n",
      "Epoch 23/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3135 - val_loss: 0.2625\n",
      "Epoch 24/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3086 - val_loss: 0.2584\n",
      "Epoch 25/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.3041 - val_loss: 0.2548\n",
      "Epoch 26/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2997 - val_loss: 0.2513\n",
      "Epoch 27/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2956 - val_loss: 0.2480\n",
      "Epoch 28/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2917 - val_loss: 0.2448\n",
      "Epoch 29/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2880 - val_loss: 0.2419\n",
      "Epoch 30/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2845 - val_loss: 0.2390\n",
      "Epoch 31/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2811 - val_loss: 0.2363\n",
      "Epoch 32/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2778 - val_loss: 0.2337\n",
      "Epoch 33/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2746 - val_loss: 0.2313\n",
      "Epoch 34/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2716 - val_loss: 0.2289\n",
      "Epoch 35/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2687 - val_loss: 0.2266\n",
      "Epoch 36/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2658 - val_loss: 0.2244\n",
      "Epoch 37/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2631 - val_loss: 0.2223\n",
      "Epoch 38/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2604 - val_loss: 0.2202\n",
      "Epoch 39/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2578 - val_loss: 0.2183\n",
      "Epoch 40/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2553 - val_loss: 0.2163\n",
      "Epoch 41/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2529 - val_loss: 0.2145\n",
      "Epoch 42/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2505 - val_loss: 0.2127\n",
      "Epoch 43/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2482 - val_loss: 0.2110\n",
      "Epoch 44/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2459 - val_loss: 0.2093\n",
      "Epoch 45/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2437 - val_loss: 0.2076\n",
      "Epoch 46/50\n",
      "844/844 [==============================] - 1s 996us/step - loss: 0.2416 - val_loss: 0.2061\n",
      "Epoch 47/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2395 - val_loss: 0.2046\n",
      "Epoch 48/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2374 - val_loss: 0.2030\n",
      "Epoch 49/50\n",
      "844/844 [==============================] - 1s 994us/step - loss: 0.2354 - val_loss: 0.2016\n",
      "Epoch 50/50\n",
      "844/844 [==============================] - 1s 1ms/step - loss: 0.2334 - val_loss: 0.2002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x21abae883a0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 딥러닝은 돌아갈 것이 많으므로 batch 사이즈 지정 필요. 64개의 이미지 한 묶음으로 처리하도록.\n",
    "model.fit(X_train_centered, y_train_onehot, batch_size = 64, epochs = 50, verbose = 1, validation_split = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eba7f615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:455: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred = model.predict_classes(X_train_centered, verbose = 0)\n",
    "y_train_pred[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b654e4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5923\n",
      "0.09871666666666666\n"
     ]
    }
   ],
   "source": [
    "total_predicts = np.sum(y_train == y_train_pred, axis = 0)\n",
    "print(total_predicts)\n",
    "train_res = total_predicts/y_train.shape[0]\n",
    "print(train_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb93c934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = model.predict_classes(X_test_centered, verbose=0)\n",
    "y_test_pred[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "baa68b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1], dtype=uint8)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "970ea014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9339\n",
      "0.9339\n"
     ]
    }
   ],
   "source": [
    "total_predicts = np.sum(y_test == y_test_pred, axis = 0)\n",
    "print(total_predicts)\n",
    "test_res = total_predicts/y_test.shape[0]\n",
    "print(test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf4e5ce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 5 6\n",
      "33 4 6\n",
      "38 2 3\n",
      "62 9 4\n",
      "63 3 2\n",
      "92 9 4\n",
      "124 7 4\n",
      "149 2 9\n",
      "195 3 5\n",
      "217 6 5\n",
      "233 8 7\n",
      "241 9 8\n",
      "245 3 5\n",
      "247 4 2\n",
      "259 6 0\n",
      "261 5 3\n",
      "290 8 4\n",
      "300 4 1\n",
      "318 2 3\n",
      "320 9 7\n",
      "321 2 7\n",
      "340 5 3\n",
      "352 5 0\n",
      "359 9 4\n",
      "362 2 7\n",
      "380 0 5\n",
      "381 3 7\n",
      "412 5 3\n",
      "435 8 7\n",
      "444 2 8\n",
      "445 6 0\n",
      "448 9 8\n",
      "449 3 5\n",
      "464 3 7\n",
      "478 5 8\n",
      "479 9 3\n",
      "495 8 3\n",
      "502 5 3\n",
      "507 3 5\n",
      "528 3 2\n",
      "531 3 6\n",
      "542 8 4\n",
      "543 8 3\n",
      "551 7 1\n",
      "565 4 9\n",
      "578 3 2\n",
      "582 8 2\n",
      "591 8 3\n",
      "610 4 6\n",
      "613 2 8\n",
      "619 1 8\n",
      "627 9 4\n",
      "628 3 9\n",
      "629 2 6\n",
      "658 7 0\n",
      "659 2 8\n",
      "684 7 3\n",
      "691 8 4\n",
      "707 4 9\n",
      "717 0 6\n",
      "720 5 8\n",
      "740 4 9\n",
      "760 4 9\n",
      "810 7 2\n",
      "839 8 3\n",
      "844 8 7\n",
      "857 5 3\n",
      "881 4 9\n",
      "882 9 7\n",
      "924 2 7\n",
      "938 3 5\n",
      "947 8 9\n",
      "950 7 2\n",
      "956 1 2\n",
      "959 4 5\n",
      "965 6 0\n",
      "975 2 3\n",
      "982 3 8\n",
      "1003 5 3\n",
      "1014 6 5\n",
      "1032 5 8\n",
      "1039 7 1\n",
      "1044 6 8\n",
      "1050 2 6\n",
      "1062 3 7\n",
      "1068 8 4\n",
      "1082 5 3\n",
      "1092 3 1\n",
      "1101 8 3\n",
      "1107 9 3\n",
      "1112 4 6\n",
      "1114 3 8\n",
      "1124 8 7\n",
      "1128 3 2\n",
      "1131 5 4\n",
      "1147 4 9\n",
      "1181 6 1\n",
      "1192 9 4\n",
      "1194 7 9\n",
      "1198 8 4\n",
      "1200 8 5\n",
      "1202 8 5\n",
      "1204 3 9\n",
      "1224 2 6\n",
      "1226 7 2\n",
      "1228 9 3\n",
      "1232 9 4\n",
      "1242 4 9\n",
      "1247 9 0\n",
      "1248 8 5\n",
      "1251 2 6\n",
      "1256 2 7\n",
      "1260 7 1\n",
      "1283 7 2\n",
      "1289 5 4\n",
      "1299 5 7\n",
      "1310 3 7\n",
      "1319 8 3\n",
      "1326 7 2\n",
      "1347 7 9\n",
      "1364 8 2\n",
      "1378 5 6\n",
      "1393 5 3\n",
      "1414 9 4\n",
      "1422 4 9\n",
      "1433 8 1\n",
      "1444 6 4\n",
      "1453 4 9\n",
      "1465 4 1\n",
      "1467 5 9\n",
      "1500 7 1\n",
      "1522 7 9\n",
      "1525 5 0\n",
      "1527 1 6\n",
      "1530 8 7\n",
      "1549 4 6\n",
      "1553 9 3\n",
      "1559 9 3\n",
      "1581 7 9\n",
      "1609 2 6\n",
      "1621 0 6\n",
      "1634 4 7\n",
      "1640 9 7\n",
      "1678 2 0\n",
      "1681 3 7\n",
      "1696 2 1\n",
      "1709 9 5\n",
      "1717 8 0\n",
      "1718 7 3\n",
      "1722 2 4\n",
      "1727 3 7\n",
      "1751 4 2\n",
      "1754 7 2\n",
      "1759 8 6\n",
      "1772 7 9\n",
      "1773 1 8\n",
      "1774 8 5\n",
      "1782 8 2\n",
      "1790 2 9\n",
      "1813 8 5\n",
      "1828 3 7\n",
      "1850 8 9\n",
      "1857 6 4\n",
      "1865 4 9\n",
      "1878 8 3\n",
      "1883 7 9\n",
      "1901 9 4\n",
      "1903 7 4\n",
      "1911 5 0\n",
      "1917 5 8\n",
      "1926 3 5\n",
      "1930 2 4\n",
      "1938 4 6\n",
      "1941 7 4\n",
      "1952 9 5\n",
      "1970 5 3\n",
      "1982 6 5\n",
      "1984 2 0\n",
      "2016 7 2\n",
      "2024 7 9\n",
      "2035 5 3\n",
      "2040 5 4\n",
      "2043 4 8\n",
      "2044 2 7\n",
      "2053 4 9\n",
      "2063 7 5\n",
      "2070 7 9\n",
      "2098 2 0\n",
      "2099 8 9\n",
      "2109 3 9\n",
      "2110 2 8\n",
      "2118 6 5\n",
      "2129 9 2\n",
      "2130 4 9\n",
      "2135 6 1\n",
      "2148 4 9\n",
      "2168 8 2\n",
      "2174 3 5\n",
      "2182 1 2\n",
      "2185 0 5\n",
      "2186 2 5\n",
      "2189 9 1\n",
      "2215 6 5\n",
      "2224 5 6\n",
      "2266 1 6\n",
      "2272 8 0\n",
      "2293 9 6\n",
      "2299 2 7\n",
      "2305 3 8\n",
      "2325 7 3\n",
      "2369 5 8\n",
      "2371 4 9\n",
      "2378 0 2\n",
      "2387 9 1\n",
      "2393 8 5\n",
      "2395 8 3\n",
      "2406 9 1\n",
      "2422 6 4\n",
      "2425 8 3\n",
      "2433 2 1\n",
      "2449 0 5\n",
      "2462 2 0\n",
      "2473 1 8\n",
      "2488 2 4\n",
      "2526 5 3\n",
      "2534 3 5\n",
      "2545 5 3\n",
      "2548 9 4\n",
      "2556 5 8\n",
      "2559 5 3\n",
      "2560 3 2\n",
      "2573 5 8\n",
      "2574 5 7\n",
      "2582 9 7\n",
      "2598 8 2\n",
      "2607 7 1\n",
      "2610 2 8\n",
      "2617 8 7\n",
      "2631 0 6\n",
      "2635 2 8\n",
      "2648 9 0\n",
      "2650 8 5\n",
      "2654 6 1\n",
      "2695 7 4\n",
      "2698 5 3\n",
      "2705 1 8\n",
      "2760 9 4\n",
      "2770 3 5\n",
      "2771 4 9\n",
      "2780 2 3\n",
      "2810 5 3\n",
      "2832 5 3\n",
      "2850 5 3\n",
      "2851 7 9\n",
      "2863 9 4\n",
      "2896 8 0\n",
      "2911 8 1\n",
      "2925 5 0\n",
      "2927 3 2\n",
      "2930 5 7\n",
      "2932 0 5\n",
      "2945 3 7\n",
      "2953 3 5\n",
      "2970 5 7\n",
      "2995 6 5\n",
      "3005 9 8\n",
      "3060 9 7\n",
      "3065 8 1\n",
      "3073 1 2\n",
      "3102 5 3\n",
      "3110 3 5\n",
      "3114 4 6\n",
      "3115 5 4\n",
      "3117 5 9\n",
      "3130 6 0\n",
      "3136 7 1\n",
      "3138 3 0\n",
      "3145 5 9\n",
      "3164 6 2\n",
      "3189 7 4\n",
      "3206 8 3\n",
      "3240 9 3\n",
      "3269 6 0\n",
      "3289 8 7\n",
      "3316 7 4\n",
      "3329 7 2\n",
      "3330 2 3\n",
      "3336 5 7\n",
      "3381 3 8\n",
      "3405 4 9\n",
      "3447 6 4\n",
      "3468 5 4\n",
      "3490 4 9\n",
      "3503 9 1\n",
      "3520 6 4\n",
      "3549 3 2\n",
      "3550 6 5\n",
      "3558 5 0\n",
      "3559 8 5\n",
      "3565 5 8\n",
      "3567 8 5\n",
      "3573 7 4\n",
      "3575 7 9\n",
      "3597 9 3\n",
      "3604 7 0\n",
      "3629 8 3\n",
      "3662 8 5\n",
      "3674 8 3\n",
      "3681 2 8\n",
      "3688 6 5\n",
      "3702 5 3\n",
      "3716 9 3\n",
      "3718 4 9\n",
      "3726 4 9\n",
      "3730 7 9\n",
      "3751 7 1\n",
      "3757 8 1\n",
      "3767 7 2\n",
      "3776 5 8\n",
      "3780 4 6\n",
      "3796 2 8\n",
      "3806 5 8\n",
      "3808 7 3\n",
      "3811 2 3\n",
      "3817 2 8\n",
      "3818 0 6\n",
      "3821 9 4\n",
      "3833 8 7\n",
      "3834 3 2\n",
      "3836 7 9\n",
      "3838 7 1\n",
      "3846 6 2\n",
      "3848 7 3\n",
      "3853 6 5\n",
      "3855 5 8\n",
      "3862 2 3\n",
      "3869 9 4\n",
      "3871 8 3\n",
      "3876 2 8\n",
      "3893 5 6\n",
      "3902 5 3\n",
      "3906 1 3\n",
      "3926 9 3\n",
      "3941 4 6\n",
      "3943 3 5\n",
      "3946 2 8\n",
      "3951 8 7\n",
      "3962 3 4\n",
      "3967 9 7\n",
      "3976 7 1\n",
      "3984 9 8\n",
      "3985 9 4\n",
      "4000 9 4\n",
      "4002 3 5\n",
      "4017 4 9\n",
      "4027 7 9\n",
      "4044 3 5\n",
      "4063 6 5\n",
      "4065 0 6\n",
      "4072 5 3\n",
      "4075 8 0\n",
      "4078 9 7\n",
      "4093 9 4\n",
      "4131 5 3\n",
      "4140 8 2\n",
      "4145 8 3\n",
      "4152 5 1\n",
      "4154 9 4\n",
      "4156 2 8\n",
      "4159 8 3\n",
      "4163 9 5\n",
      "4176 2 4\n",
      "4177 5 4\n",
      "4199 7 9\n",
      "4201 1 7\n",
      "4205 2 1\n",
      "4211 6 5\n",
      "4212 1 3\n",
      "4224 9 7\n",
      "4228 7 2\n",
      "4238 7 9\n",
      "4248 2 8\n",
      "4256 3 2\n",
      "4260 2 3\n",
      "4271 5 3\n",
      "4289 2 7\n",
      "4300 5 9\n",
      "4306 3 7\n",
      "4313 4 9\n",
      "4315 5 8\n",
      "4344 9 4\n",
      "4355 5 9\n",
      "4356 5 8\n",
      "4369 9 4\n",
      "4374 5 6\n",
      "4380 8 5\n",
      "4415 2 0\n",
      "4425 9 4\n",
      "4433 7 3\n",
      "4435 3 7\n",
      "4449 6 0\n",
      "4451 2 8\n",
      "4455 8 0\n",
      "4497 8 7\n",
      "4498 7 8\n",
      "4523 8 3\n",
      "4534 9 8\n",
      "4540 7 9\n",
      "4552 3 5\n",
      "4571 6 2\n",
      "4575 4 2\n",
      "4578 7 9\n",
      "4601 8 4\n",
      "4615 2 4\n",
      "4639 8 9\n",
      "4640 8 7\n",
      "4731 8 7\n",
      "4735 9 4\n",
      "4740 3 5\n",
      "4751 4 6\n",
      "4761 9 7\n",
      "4763 5 6\n",
      "4785 3 8\n",
      "4807 8 3\n",
      "4808 3 5\n",
      "4814 6 0\n",
      "4823 9 4\n",
      "4829 8 3\n",
      "4837 7 2\n",
      "4852 8 6\n",
      "4860 4 9\n",
      "4874 9 6\n",
      "4876 2 6\n",
      "4879 8 4\n",
      "4880 0 8\n",
      "4886 7 1\n",
      "4890 8 2\n",
      "4910 9 4\n",
      "4915 5 8\n",
      "4939 2 3\n",
      "4950 2 3\n",
      "4956 8 4\n",
      "4966 7 1\n",
      "4990 3 8\n",
      "5001 9 4\n",
      "5011 8 1\n",
      "5038 3 2\n",
      "5067 3 2\n",
      "5068 4 1\n",
      "5078 3 8\n",
      "5140 3 0\n",
      "5183 8 4\n",
      "5210 9 7\n",
      "5331 1 6\n",
      "5401 6 5\n",
      "5495 8 3\n",
      "5562 2 8\n",
      "5600 7 9\n",
      "5611 8 1\n",
      "5620 7 9\n",
      "5642 1 5\n",
      "5678 8 5\n",
      "5688 7 9\n",
      "5714 7 9\n",
      "5734 3 7\n",
      "5749 8 6\n",
      "5771 8 6\n",
      "5780 3 2\n",
      "5821 5 3\n",
      "5835 7 9\n",
      "5842 4 7\n",
      "5874 5 3\n",
      "5887 7 3\n",
      "5888 4 0\n",
      "5891 5 2\n",
      "5913 5 3\n",
      "5922 5 3\n",
      "5936 4 9\n",
      "5937 5 3\n",
      "5955 3 8\n",
      "5972 5 3\n",
      "5973 3 5\n",
      "5985 5 8\n",
      "6023 3 5\n",
      "6035 2 0\n",
      "6042 5 3\n",
      "6043 5 3\n",
      "6045 3 9\n",
      "6059 3 5\n",
      "6065 3 8\n",
      "6071 9 3\n",
      "6081 9 8\n",
      "6091 9 0\n",
      "6157 9 0\n",
      "6166 9 3\n",
      "6172 9 0\n",
      "6173 9 0\n",
      "6347 8 6\n",
      "6385 5 2\n",
      "6391 2 6\n",
      "6392 5 8\n",
      "6400 0 6\n",
      "6421 3 2\n",
      "6425 6 2\n",
      "6432 3 8\n",
      "6505 9 0\n",
      "6555 8 7\n",
      "6560 9 3\n",
      "6564 3 7\n",
      "6568 9 4\n",
      "6571 9 7\n",
      "6597 0 7\n",
      "6598 5 2\n",
      "6603 8 7\n",
      "6625 8 7\n",
      "6632 9 5\n",
      "6641 8 5\n",
      "6645 2 8\n",
      "6651 0 5\n",
      "6706 5 7\n",
      "6721 2 8\n",
      "6740 9 0\n",
      "6744 2 8\n",
      "6746 5 4\n",
      "6769 4 8\n",
      "6775 5 1\n",
      "6785 2 4\n",
      "6796 2 7\n",
      "6817 9 4\n",
      "6847 6 4\n",
      "6906 2 6\n",
      "6926 6 4\n",
      "6981 5 6\n",
      "7043 9 7\n",
      "7107 9 7\n",
      "7121 8 9\n",
      "7130 3 9\n",
      "7198 8 9\n",
      "7216 0 7\n",
      "7220 8 3\n",
      "7233 3 7\n",
      "7241 5 8\n",
      "7265 8 3\n",
      "7338 4 9\n",
      "7432 7 1\n",
      "7434 4 8\n",
      "7451 5 6\n",
      "7459 9 5\n",
      "7498 5 3\n",
      "7539 2 8\n",
      "7545 8 9\n",
      "7603 8 5\n",
      "7637 2 3\n",
      "7797 5 6\n",
      "7800 3 2\n",
      "7812 1 8\n",
      "7821 3 2\n",
      "7839 1 8\n",
      "7842 5 8\n",
      "7849 3 2\n",
      "7850 5 8\n",
      "7851 6 0\n",
      "7858 3 2\n",
      "7859 5 4\n",
      "7870 5 4\n",
      "7886 2 4\n",
      "7888 5 4\n",
      "7918 5 8\n",
      "7945 2 6\n",
      "7990 1 8\n",
      "8020 1 8\n",
      "8062 5 8\n",
      "8072 5 3\n",
      "8081 4 6\n",
      "8091 2 8\n",
      "8094 2 8\n",
      "8095 4 8\n",
      "8165 2 4\n",
      "8183 8 5\n",
      "8245 2 7\n",
      "8246 3 8\n",
      "8272 3 8\n",
      "8277 3 5\n",
      "8279 8 6\n",
      "8294 8 5\n",
      "8308 3 5\n",
      "8332 9 7\n",
      "8339 8 6\n",
      "8353 2 4\n",
      "8397 3 0\n",
      "8408 8 4\n",
      "8410 8 6\n",
      "8426 9 7\n",
      "8476 8 5\n",
      "8520 4 9\n",
      "8522 8 6\n",
      "9009 7 2\n",
      "9010 2 8\n",
      "9015 7 2\n",
      "9016 0 5\n",
      "9019 7 2\n",
      "9024 7 2\n",
      "9036 7 2\n",
      "9045 7 2\n",
      "9071 1 8\n",
      "9182 3 9\n",
      "9211 4 9\n",
      "9214 9 4\n",
      "9245 9 7\n",
      "9280 8 5\n",
      "9316 8 9\n",
      "9385 8 6\n",
      "9422 5 3\n",
      "9446 2 6\n",
      "9465 5 3\n",
      "9482 5 3\n",
      "9544 9 7\n",
      "9587 9 4\n",
      "9595 2 8\n",
      "9614 3 5\n",
      "9624 3 8\n",
      "9634 0 3\n",
      "9642 9 7\n",
      "9643 1 7\n",
      "9679 6 3\n",
      "9698 6 5\n",
      "9700 2 8\n",
      "9716 2 0\n",
      "9719 5 0\n",
      "9729 5 6\n",
      "9733 9 8\n",
      "9744 8 1\n",
      "9745 4 2\n",
      "9749 5 0\n",
      "9752 2 0\n",
      "9768 2 0\n",
      "9770 5 0\n",
      "9777 5 0\n",
      "9779 2 0\n",
      "9808 9 4\n",
      "9811 2 8\n",
      "9839 2 7\n",
      "9856 9 4\n",
      "9858 6 3\n",
      "9867 2 8\n",
      "9879 0 6\n",
      "9888 6 0\n",
      "9890 9 4\n",
      "9891 9 7\n",
      "9892 8 6\n",
      "9893 2 8\n",
      "9905 3 7\n",
      "9940 6 0\n",
      "9941 5 3\n",
      "9943 3 8\n",
      "9944 3 9\n",
      "9959 8 7\n",
      "9970 5 3\n",
      "9975 3 2\n",
      "9982 5 6\n",
      "9986 3 8\n",
      "661\n"
     ]
    }
   ],
   "source": [
    "n = 0\n",
    "for i, y in enumerate(y_test):\n",
    "    if y != y_test_pred[i]:\n",
    "        print(i, y, y_test_pred[i])\n",
    "        n+=1\n",
    "        \n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3cdb233b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (5, 5))\n",
    "image = np.reshape(X_test[1204], [28, 28])\n",
    "plt.imshow(image, cmap = \"Greys\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "326c20fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN (Convolutional Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "57ea26f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (5, 5), padding = 'valid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6b82b497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 24, 24, 32)        832       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 64)          51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 1,111,946\n",
      "Trainable params: 1,111,946\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (5, 5), padding = 'valid', activation = 'relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPool2D(2, 2))\n",
    "model.add(layers.Conv2D(64, (5, 5), padding = 'valid', activation = 'relu'))\n",
    "model.add(layers.MaxPool2D(2,2))\n",
    "model.add(layers.Flatten())\n",
    "# 2d 를 flat으로 다 펴버림.\n",
    "# 이제 일반 모델처럼 된 것. (이미지 => 데이터 형태로 들어옴.)\n",
    "\n",
    "model.add(layers.Dense(1024, activation = 'relu'))\n",
    "model.add(layers.Dropout(0.5))                       # overfit 방지(50% 학습을 안 시키는 것.)\n",
    "model.add(layers.Dense(10, activation = 'softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5518a0af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
